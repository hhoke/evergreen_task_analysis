<script type="text/javascript" charset="utf-8" 
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,
https://vincenttam.github.io/javascripts/MathJaxLocal.js"></script>

# Evergreen Performance Bulletin 2021-06-14

As always, please let me know if you have any suggestions whatsoever.

Link to the [last](https://hhoke.github.io/evergreen_task_analysis/2021-03-22.html) and [next](https://hhoke.github.io/evergreen_task_analysis/2021-06-14.html) bulletins.

## Updates

### New Epic: Structured, Granular Alerting with Prometheus Alert Manager

This will address many of the alerting difficulties outlined in prior newsletters. Scope is approved, and the epic is [second up to execute](https://docs.google.com/document/d/1RRWS0auwTZ6PyeC43p-8QIvdE2ilJHLI5Jrqvta4TIc/edit#heading=h.b1os3ai9s8t3).

#### fixing idle time metric

[Idle time metric is fixed.](https://jira.mongodb.org/browse/EVG-14363)

### Host Drawdown Job Reduces Idle Time ~5x in Week-long rhel80-small Test

See https://github.com/evergreen-ci/evergreen/pull/4633 for software implementation details. 
A host drawdown job is created for a particular distro when that distro is very overallocated (host queue ratio < .25), and the job has safeguards to make sure that hosts will only be terminated until the distro is near a host queue ratio of 1.
We further require that hosts be idle for 30 seconds or more, which should give enough time for them to pick up new tasks if any exist (especially considering the fact that GetNextTask will run quickly on a near-empty queue).
Drawdown jobs will be created until the host queue ration rises above .25.
This heuristic is almost certainly suboptimal, but .25 has worked well as a first estimate.

As a test of this new functionality, I went to the rhel80-small distro page and set the Host Overallocation Rule to "terminate-hosts-when-overallocated" on Thursday, June 10th.

In order to get a rough initial estimate of the effect of the termination jobs, I compared two queue-exhaustion events, one before I turned on the feature and one after.

Looking at the first event, we started at just under 1000 hosts. 
Five minutes after the queue emptied (roughly), we had 375 idle machines.
For this second event, we started out at just over 1000 hosts, and pre-emptively killed about 400 hosts, leaving us with about 35 idle hosts five minutes after the queue emptied. 

I then let the distro run normally for about a week.
The initial results were borne out by further data.

The reduction in idle time is plainly visible on this chart:

![idle time visibly reduced](https://evergreen-task-analysis.s3.us-east-2.amazonaws.com/IdleDiffrhel80small2021-06-17at17.30.35.png)

However, this overstates things somewhat, because more hosts were spun up as a result of greater termination propensity.
The three Fridays prior to the change averaged to 8,487 startups.
Friday the 11th had 9,263 host startups, for a +776 difference.
Evergreen devs have told me that the average time to start up a host is 2 minutes, and a brief spot check confirmed this is [roughly](https://evergreen.mongodb.com/host/i-01efdf6a4b326bcce) [correct](https://evergreen.mongodb.com/event_log/host/i-0ad81bc52f713e853).
This gives us 1,552 extra startup minutes.

The average summed idle time, over the last four Fridays, was 60 thousand minutes.
UTC Friday 11th Jun has a summed idle time of 9,309 minutes, with an estimated 1,552 extra startup minutes, for a roughly 5.5x reduction.

With an instance type of m4.xlarge, spot instances are currently $0.0663 per minute in NVA.
This new feature therefore provides, in the operation of rhel80-small, a savings of roughly 

$$(\frac{\$0.0663} {\text{Hour in N. Virginia}}) * (\frac{1 \text{ hr}} {60\text{ minutes}}) * (\frac{60000-10761\text{ minutes}}{\text{day}})  \approx \frac{\$54}{day} $$

more generally,

$$(\frac{\$\text{price}} {\text{Hour in N. Virginia}}) * (\frac{1 \text{ hr}} {60\text{ minutes}}) * (\frac{\text{task runtime minutes}}{\text{day}}) * \frac{4.5}{5.5} \approx \frac{\$X\text{ saved}}{day} $$

This represents roughly $17k/yr savings for the operation of rhel80-small, based on an average idle time per day of 50,784 over the last 30 days.

The current spot price of m4.2xlarge is $0.1924 per hour, which gives $48.6k/yr by the above equation.
As load is largely similar between the two distros, this should mean that we could save around $65k/yr using this feature just on these two distros, assuming we use spot and never on-demand. This is only an initial estimate, but is conservative (likely an underestimate).

Consequently, I propose we turn on this feature by default for all ephemeral, by-the-minute distros.

#### Minimal QoS/Task Wait Impact

Despite normal levels of task activity, the average number of overdue tasks was not noticeably higher after this feature was turned on.
If anything, it appears lower, but this is probably just noise.

There is a similar trend in 99th percentile `time_to_start_from_unblocked`.

NOTE: weeklies imply that extra spin-up is around 1k hosts / day

#### experimentation standards

In the future, power analyses may be useful, as mentioned in earlier bulletins.
For example, a power analysis might be useful to determine if I looked at enough data to detect a difference in performance, if one existed.
However, for the current analysis I want to move quickly.
The effect of host drawdowns on idle time was dramatic enough that I'm confident this doesn't represent a random aberration, and am similarly confident that performance impacts have been minimal.



## Future Directions

### Understanding Idle Time Tradeoffs

This is the most important new area of inquiry, IMO. That is to say, it has the greatest expected value for return on information.

There are three main reasons we need to better understand the tradeoffs associated with host idleness:

1. Savings: The host drawdown approach above should take care of roughly 4/5ths of the idle host time we currently see. As such, we can consider the low-hanging fruit picked and move on to other topics for now.
2. Windows Performance: We currently regulate windows host costs by setting low distro-level max hosts limits. This results in very bad performance. It might be possible to get equivalent cost savings without as heavy as a toll on performance. 
3. Making sure performance increases aren't too costly: In general, I think it is a good idea to have some kind of feedback rule that increases host allocation as a response to poor performance (e.g. time since queue was last cleared, number of long-wait tasks). However, this will likely result in greater idle time. Before we take any action that significantly increases the host idle time, we should understand the tradeoffs.

I've begun to look into this. Some important facts and preliminary work:

## Follow Up On [EVG-14350](https://jira.mongodb.org/browse/EVG-14350).

Based on [previous work](https://hhoke.github.io/evergreen_task_analysis/2021-03-08.html), I decided a heuristic based on a low-enough value of host queue ratio would allow us to decrease idle time by a lot, while avoiding impacts on performance.
Pending a more rigorous analysis, this appears to be correct.

#### Task Packing to decrease idle host time for Windows

I've been looking at windows-64-vs2017-large more closely, on account of its poor performance.
There are idle hosts up to 2 hours after the queue is exhausted.
Currently, hosts request tasks as soon as they are free, which guarantees that the task at the head of the queue is taken off the queue in the fastest time possible.
However, in the case of windows distros, the performance is degraded to the point of multiple-hour waits, due to the distro max hosts limit.
We should modify our strategy, because it makes no sense to mind seconds and let hours go by.

We are currently able to predict when a queue is in terminal decline, using the new host queue ratio metric.
As such, if we were able to use some coordination step that added about a minute to each task's wait, without worrying about modes, it would still be worth it if we were able to decrease idle time enough to justify the lifting of distro max host limits.
However, this could involve significant reworking of the task scheduling framework.

Another possibility would be to mark hosts nearing the end of their allotted hour as unable to accept new tasks, if the host queue ratio is low enough.

A third possibility is to have hosts include the amount of time they have left in their current hour when requesting tasks, and only accept tasks that are less than that amount of time. 

This needs to be thought out more before it is implemented.

### Future Directions Not Considered

~~~

## Blotter

*(For one-off or temporary performance issues)*


### Daily Task Level Not Related to Variables of Interest

From eyeballing rhel80-small charts, I think total daily task level is not very related to idle host count, host start count, tasks overdue or perc99 task wait.
I think some function of a momentary variable, like # of tasks added per period, would be more insightful.
There is likely some relationship to burstiness, total number of new tasks per hour/minute, and total system load that would have a closer relationship with these variables of interest.

